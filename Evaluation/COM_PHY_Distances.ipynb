{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import construction as cs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### read font\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_dirs = ['Barlow/']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "\n",
    "# set font\n",
    "plt.rcParams['font.family'] = 'Barlow'\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from topological_metrics import *\n",
    "\n",
    "import os\n",
    "\n",
    "# example of calculating the kl divergence between two mass functions\n",
    "from math import log2\n",
    "from scipy.stats import wasserstein_distance as em\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different distances topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(data, n=10): \n",
    "    data = np.array(data)\n",
    "    h, e = np.histogram(data, n)\n",
    "    p = h/data.shape[0]\n",
    "    return e, p\n",
    "\n",
    "def support_intersection(p, q): \n",
    "    sup_int = (\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: (x[0]!=0) & (x[1]!=0), zip(p, q)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return sup_int\n",
    "\n",
    "def get_probs(list_of_tuples): \n",
    "    p = np.array([p[0] for p in list_of_tuples])\n",
    "    q = np.array([p[1] for p in list_of_tuples])\n",
    "    return p, q\n",
    "\n",
    "def kl_divergence(p, q): \n",
    "    return np.sum(p*np.log(p/q))\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = (1./2.)*(p + q)\n",
    "    return (1./2.)*kl_divergence(p, m) + (1./2.)*kl_divergence(q, m)\n",
    "\n",
    "def compute_kl_divergence(train_sample, test_sample, n_bins=10,js=False): \n",
    "    \"\"\"\n",
    "    Computes the KL Divergence using the support \n",
    "    intersection between two different samples\n",
    "    \"\"\"\n",
    "    E = 0.0000000001\n",
    "    e, p = compute_probs(train_sample, n=n_bins)\n",
    "    _, q = compute_probs(test_sample, n=e)\n",
    "    \n",
    "    p = np.array(p) + E \n",
    "    q = np.array(q) + E \n",
    "    \n",
    "    p = p/sum(p)\n",
    "    q = q/sum(q)\n",
    "    \n",
    "    list_of_tuples = support_intersection(p, q)\n",
    "    p, q = get_probs(list_of_tuples)\n",
    "    \n",
    "    if js:\n",
    "        return js_divergence(p, q)\n",
    "    else:\n",
    "        return kl_divergence(p, q)\n",
    "def comp_stat(ori,competitor,dist,names):\n",
    "    res = dict()\n",
    "    c = 0\n",
    "    for met in competitor:\n",
    "        tmp = []\n",
    "        for comp in met:\n",
    "            if dist == \"js\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=True)\n",
    "            elif dist == \"kl\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=False)\n",
    "            elif dist == \"em\":\n",
    "                val = em(ori[c][0],comp)\n",
    "            elif dist == \"ks\":\n",
    "                val = (ks_2samp(ori[c][0],comp)[0])\n",
    "                \n",
    "            tmp.append(val)\n",
    "            \n",
    "        res[names[c]] = (np.mean(tmp),np.std(tmp))\n",
    "        c = c + 1 \n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_comp_metric(file_name,dist=\"ks\"):\n",
    "    o_in = load_topo_original(file_name)\n",
    "    e_in = load_topo_distributions(\"etngen\",file_name)\n",
    "    t_in = load_topo_distributions(\"taggen\",file_name)\n",
    "    d_in = load_topo_distributions(\"dymgen\",file_name)\n",
    "    s_in = load_topo_distributions(\"stmgen\",file_name)\n",
    "\n",
    "    res_e = comp_stat(o_in,e_in,dist= dist,names=names)\n",
    "    res_d = comp_stat(o_in,d_in,dist= dist,names=names)\n",
    "    res_s = comp_stat(o_in,s_in,dist= dist,names=names)\n",
    "    res_t = comp_stat(o_in,t_in,dist= dist,names=names)\n",
    "\n",
    "    x1 = np.array(list(res_e.values()))\n",
    "    x2 = np.array(list(res_s.values()))\n",
    "    x3 = np.array(list(res_t.values()))\n",
    "    x4 = np.array(list(res_d.values()))\n",
    "    \n",
    "    return x1,x2,x3,x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topo_distributions(generator,file_name):\n",
    "    \n",
    "    den = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/density.npy\",allow_pickle=True)\n",
    "    inter_indiv = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/interacting_indiv.npy\",allow_pickle=True)\n",
    "    new_conv = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/new_con.npy\",allow_pickle=True)\n",
    "    durat = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/dur.npy\",allow_pickle=True)\n",
    "    clust = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/clust.npy\",allow_pickle=True)\n",
    "    #s_met = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/s_metric.npy\",allow_pickle=True)\n",
    "    ass = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/ass.npy\",allow_pickle=True)\n",
    "    #asp = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/asp.npy\",allow_pickle=True)\n",
    "    hclose = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hclose.npy\",allow_pickle=True)\n",
    "    hbet = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hbet.npy\",allow_pickle=True)\n",
    "    whbet = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/whbet.npy\",allow_pickle=True)\n",
    "    conncomp = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/conncomp.npy\",allow_pickle=True)\n",
    "    hmod = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hmod.npy\",allow_pickle=True)\n",
    "    \n",
    "    hs_met = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hs_metric.npy\",allow_pickle=True)\n",
    "    hasp = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hasp.npy\",allow_pickle=True)\n",
    "    #nb_inter = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/nb_interactions.npy\",allow_pickle=True)\n",
    "    #stren = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/streng.npy\",allow_pickle=True \n",
    "\n",
    "    #return den,inter_indiv,new_conv,durat,clust,ass,hclose,hbet,whbet,conncomp,hmod,hs_met,hasp\n",
    "    return den,inter_indiv,new_conv,durat,clust,ass,conncomp,hclose,hbet,whbet,hmod,hs_met,hasp\n",
    "\n",
    "def load_topo_original(file_name):\n",
    "    den = np.load(\"topology_results/original_distributions/\"+file_name+\"/density.npy\",allow_pickle=True)\n",
    "    inter_indiv = np.load(\"topology_results/original_distributions/\"+file_name+\"/interacting_indiv.npy\",allow_pickle=True)\n",
    "    new_conv = np.load(\"topology_results/original_distributions/\"+file_name+\"/new_con.npy\",allow_pickle=True)\n",
    "    durat = np.load(\"topology_results/original_distributions/\"+file_name+\"/dur.npy\",allow_pickle=True)\n",
    "    clust = np.load(\"topology_results/original_distributions/\"+file_name+\"/clust.npy\",allow_pickle=True)\n",
    "    ass = np.load(\"topology_results/original_distributions/\"+file_name+\"/ass.npy\",allow_pickle=True)\n",
    "    hclose = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hclose.npy\",allow_pickle=True)\n",
    "    hbet = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hbet.npy\",allow_pickle=True)\n",
    "    whbet = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/whbet.npy\",allow_pickle=True)\n",
    "    conncomp = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/conncomp.npy\",allow_pickle=True)\n",
    "    hmod = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hmod.npy\",allow_pickle=True)\n",
    "    \n",
    "    hs_met = np.load(\"topology_results/original_distributions/\"+file_name+\"/hs_metric.npy\",allow_pickle=True)\n",
    "    hasp = np.load(\"topology_results/original_distributions/\"+file_name+\"/hasp.npy\",allow_pickle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #s_met = np.load(\"topology_results/original_distributions/\"+file_name+\"/s_metric.npy\",allow_pickle=True)\n",
    "    #asp = np.load(\"topology_results/original_distributions/\"+file_name+\"/asp.npy\",allow_pickle=True)\n",
    "    #stren = np.load(\"topology_results/original_distributions/\"+file_name+\"/streng.npy\",allow_pickle=True)\n",
    "    #nb_inter = np.load(\"topology_results/original_distributions/\"+file_name+\"/nb_interactions.npy\",allow_pickle=True)\n",
    "    \n",
    "    return den,inter_indiv,new_conv,durat,clust,ass,[conncomp],[hclose],[hbet],[whbet],[hmod],[hs_met],[hasp]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Density\",\"Interacting  individuals\",\"New conversations\",\"Duration of contacts\",\"Global clustering coefficient\",\n",
    "         \"Assortativity\",\"Nb connected components\",\"Hour closeness\",\"Hour betweenness\",\"Weighted hour betweenness\",\n",
    "         \"Hour modularity\",\"Hour S-metric\",\"Hour average shortestpath length\"]\n",
    "\n",
    "# etn stm tag dym\n",
    "#x1w,x2w,x3w,x4w = load_comp_metric(\"InVS13\",dist=dist)\n",
    "#x1s,x2s,x3s,x4s = load_comp_metric(\"High_School11\",dist=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name  = \"High_School11\"\n",
    "\n",
    "\n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"ks\")\n",
    "tmp = []\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp.append([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"js\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    cc = cc + 1\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"kl\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    cc = cc + 1\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"em\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.5f}\".format(a),\"{:.5f}\".format(b),\"{:.5f}\".format(c),\"{:.5f}\".format(d)])\n",
    "    cc = cc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names2 = [\"Density \",\"Int. ind.\",\"New conv.\",\"Dur.\",\"GCC\",\"Ass.\",\"Con. com.\",\"H. clos.\",\"H betw.\",\"W. h. betw.\",\n",
    "\"H. modu.\"   ,\n",
    "\"H. S-met.\"  ,\n",
    "\"H. aspl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in tmp:\n",
    "    s = names2[c]+\"&\"\n",
    "    for j in i:\n",
    "        s = s + j + \" & \"\n",
    "    c = c +1 \n",
    "    print(s[:-2]+\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.08 & 0.07 & 0.05 & 0.38 \\\\\n",
    "0.09 & 0.07 & 0.06 & 0.37 \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different distances dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_r0(file_name,lambds=[\"la001\"],picco=\"\"):\n",
    "    ori = []\n",
    "    stb = []\n",
    "    etn = []\n",
    "    tag = []\n",
    "    stm = []\n",
    "    dym = []\n",
    "    for lambd in lambds:\n",
    "        ori.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/orig.npy\"))\n",
    "        stb.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/stab.npy\"))\n",
    "        etn.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/etn.npy\"))\n",
    "        tag.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/tag.npy\"))\n",
    "        stm.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/stm.npy\"))\n",
    "        dym.append(np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/R0\"+picco+\"/\"+lambd+\"/dym.npy\"))\n",
    "        \n",
    "    #return ori,stb,etn,stm,tag,dym\n",
    "    #ori,stb,etn,tag,dym stm\n",
    "    return ori,stb,etn,tag,dym,stm\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# example of calculating the kl divergence between two mass functions\n",
    "from math import log2\n",
    "from scipy.stats import wasserstein_distance as em\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "\n",
    "def comp_stat(ori,competitor,dist,names):\n",
    "    res = dict()\n",
    "    c = 0\n",
    "    for met in competitor:\n",
    "        tmp = []\n",
    "        for comp in met:\n",
    "            if dist == \"js\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=True)\n",
    "            elif dist == \"kl\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=False)\n",
    "            elif dist == \"em\":\n",
    "                val = em(ori[c],comp)\n",
    "            elif dist == \"ks\":\n",
    "                val = (ks_2samp(ori[c],comp)[0])\n",
    "                \n",
    "            tmp.append(val)\n",
    "            \n",
    "        res[names[c]] = (np.mean(tmp),np.std(tmp))\n",
    "        c = c + 1 \n",
    "        \n",
    "    return res\n",
    "def comp_stat_stb(ori,competitor,dist,names):\n",
    "    \n",
    "    res = dict()\n",
    "    c = 0\n",
    "    for comp in competitor:\n",
    "        tmp = []\n",
    "        if dist == \"js\":\n",
    "            val = compute_kl_divergence(ori[c], comp, n_bins=50,js=True)\n",
    "        elif dist == \"kl\":\n",
    "            val = compute_kl_divergence(ori[c], comp, n_bins=50,js=False)\n",
    "        elif dist == \"em\":\n",
    "            val = em(ori[c],comp)\n",
    "        elif dist == \"ks\":\n",
    "            val = (ks_2samp(ori[c],comp)[0])\n",
    "\n",
    "        tmp.append(val)\n",
    "            \n",
    "        res[names[c]] = (np.mean(tmp),np.std(tmp))\n",
    "        c = c + 1 \n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la025,0.18106990264995987,0.2376599615209426,0.4363356677657082,0.49955667801775644,3.4256639340838193,5.283180315533095,14.360754954185321,13.731392306191896,4.545,20.52,32.230000000000004,36.997,0.146,0.3833333333333333,0.7629999999999999,0.7729999999999999,\n",
      "la015,0.13061424161995655,0.23973545146277644,0.33270988451717787,0.39472842615291076,3.476189364150686,6.736364025321502,11.508531470981861,12.270582005630617,3.8480000000000003,27.145,20.094,25.884000000000004,0.12100000000000002,0.4833333333333334,0.6289999999999999,0.7160000000000001,\n",
      "la001,0.016631256598786538,0.06480078077021854,0.01765966924456175,0.06488015373155796,0.44415139983001006,0.25529366504985906,0.25684776305576723,1.668175898623463,0.15000000000000008,11.094999999999999,0.15400000000000005,0.46900000000000003,0.064,0.47000000000000003,0.08800000000000001,0.253,\n",
      "_metala025,0.3739407895060446,0.3550151792475732,0.5303816066941316,0.3941640773839743,9.53110055895781,9.207436198853436,16.107462241569152,12.05335077004872,23.36,17.98,38.21000000000001,28.259999999999994,0.5900000000000001,0.415,0.86,0.71,\n",
      "_metala015,0.3154018608946151,0.3377960057596785,0.49368260126748476,0.2885395282377659,7.6823083040625155,7.099154391442648,15.244572634423857,9.330942083683606,16.805,14.325,28.759999999999998,19.245,0.51,0.485,0.835,0.53,\n",
      "_metala001,0.07329245293341266,0.04730644781849996,0.1603703038994675,0.04906545176644654,0.5128627239037884,0.2128461664116419,3.2889667218668035,0.38256206068150433,0.46499999999999997,5.8950000000000005,0.685,0.36,0.33999999999999997,0.29000000000000004,0.46499999999999997,0.27,\n",
      "_2piccola025,0.2605819937957618,0.23175882865543523,0.40591002035204804,0.36029581450486087,7.107155244843357,5.441819714879045,12.608815250776575,12.065331693246115,16.400000000000002,16.865,35.790000000000006,30.74,0.28,0.33499999999999996,0.765,0.665,\n",
      "_2piccola015,0.19127913253373524,0.20566996477028726,0.44749065822795747,0.32171444144076855,3.7958423029539237,4.671493456280981,14.576508205409155,9.721759879962693,11.385000000000002,19.244999999999997,29.139999999999997,21.225,0.28500000000000003,0.365,0.755,0.5349999999999999,\n",
      "_2piccola001,0.016607240133507683,0.023026108918383328,0.07693246198035551,0.015559603919211176,0.25202694668622444,0.5935656872027776,1.7729516063002775,0.23423281333036108,0.2350000000000001,8.430000000000001,0.5650000000000001,0.275,0.095,0.35000000000000003,0.29500000000000004,0.13,\n"
     ]
    }
   ],
   "source": [
    "file_name = \"High_School11\"\n",
    "lambs_all = [\"la025\",\"la015\",\"la001\"]\n",
    "picchi = [\"\",\"_meta\",\"_2picco\"]\n",
    "dist =\"ks\"\n",
    "\n",
    "\n",
    "for picco in picchi:\n",
    "    for lambs in lambs_all:\n",
    "        k = lambs\n",
    "        lambs = [lambs]\n",
    "        ori,stb,etn,tag,dym,stm = load_r0(file_name,lambds=lambs,picco=picco)\n",
    "        \n",
    "        s = picco\n",
    "        s += k+\",\"\n",
    "        dist = \"js\"\n",
    "        r_st = comp_stat_stb(ori,stb,dist,lambs)\n",
    "        r_e = comp_stat(ori,etn,dist,lambs)\n",
    "        r_t = comp_stat(ori,tag,dist,lambs)\n",
    "        r_d = comp_stat(ori,dym,dist,lambs)\n",
    "        r_s = comp_stat(ori,stm,dist,lambs)\n",
    "\n",
    "        \n",
    "        s += str(r_e[k][0])+\",\"\n",
    "        s += str(r_t[k][0])+\",\"\n",
    "        s += str(r_s[k][0])+\",\"\n",
    "        s += str(r_d[k][0])+\",\" \n",
    "\n",
    "\n",
    "        dist = \"kl\"\n",
    "        r_st = comp_stat_stb(ori,stb,dist,lambs)\n",
    "        r_e = comp_stat(ori,etn,dist,lambs)\n",
    "        r_t = comp_stat(ori,tag,dist,lambs)\n",
    "        r_d = comp_stat(ori,dym,dist,lambs)\n",
    "        r_s = comp_stat(ori,stm,dist,lambs)\n",
    "        s += str(r_e[k][0])+\",\"\n",
    "        s += str(r_t[k][0])+\",\"\n",
    "        s += str(r_s[k][0])+\",\"\n",
    "        s += str(r_d[k][0])+\",\" \n",
    "\n",
    "        \n",
    "        dist = \"em\"\n",
    "        r_st = comp_stat_stb(ori,stb,dist,lambs)\n",
    "        r_e = comp_stat(ori,etn,dist,lambs)\n",
    "        r_t = comp_stat(ori,tag,dist,lambs)\n",
    "        r_d = comp_stat(ori,dym,dist,lambs)\n",
    "        r_s = comp_stat(ori,stm,dist,lambs)\n",
    "        s += str(r_e[k][0])+\",\"\n",
    "        s += str(r_t[k][0])+\",\"\n",
    "        s += str(r_s[k][0])+\",\"\n",
    "        s += str(r_d[k][0])+\",\" \n",
    "\n",
    "        dist = \"ks\"\n",
    "        r_st = comp_stat_stb(ori,stb,dist,lambs)\n",
    "        r_e = comp_stat(ori,etn,dist,lambs)\n",
    "        r_t = comp_stat(ori,tag,dist,lambs)\n",
    "        r_d = comp_stat(ori,dym,dist,lambs)\n",
    "        r_s = comp_stat(ori,stm,dist,lambs)\n",
    "        s += str(r_e[k][0])+\",\"\n",
    "        s += str(r_t[k][0])+\",\"\n",
    "        s += str(r_s[k][0])+\",\"\n",
    "        s += str(r_d[k][0])+\",\" \n",
    "\n",
    "        print(s)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cov(file_name,p):\n",
    "    ori_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/orig.npy\")\n",
    "    sta_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/stab.npy\")\n",
    "    etn_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/etn.npy\")\n",
    "    stm_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/stm.npy\")\n",
    "    tag_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/tag.npy\")\n",
    "    dym_cov = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/coverage\"+p+\"/dym.npy\")\n",
    "    \n",
    "    return ori_cov,sta_cov,etn_cov,stm_cov,tag_cov,dym_cov\n",
    "\n",
    "def load_mfpt(file_name,p):\n",
    "    ori_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/orig.npy\")\n",
    "    sta_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/stab.npy\")\n",
    "    etn_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/etn.npy\",allow_pickle=True)\n",
    "    stm_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/stm.npy\",allow_pickle=True)\n",
    "    tag_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/tag.npy\",allow_pickle=True)\n",
    "    dym_mfpt = np.load(\"dynamic_results/\"+file_name+\"/Multiple_run/MFPT\"+p+\"/dym.npy\",allow_pickle=True)\n",
    "    \n",
    "    return ori_mfpt,sta_mfpt,etn_mfpt,stm_mfpt,tag_mfpt,dym_mfpt\n",
    "\n",
    "\n",
    "def comp_stat_stb(ori,competitor,dist,names):\n",
    "    \n",
    "    res = dict()\n",
    "    c = 0\n",
    "    for comp in competitor:\n",
    "        tmp = []\n",
    "        if dist == \"js\":\n",
    "            val = compute_kl_divergence(ori[c], comp, n_bins=50,js=True)\n",
    "        elif dist == \"kl\":\n",
    "            val = compute_kl_divergence(ori[c], comp, n_bins=50,js=False)\n",
    "        elif dist == \"em\":\n",
    "            val = em(ori[c],comp)\n",
    "        elif dist == \"ks\":\n",
    "            val = (ks_2samp(ori[c],comp)[0])\n",
    "\n",
    "        tmp.append(val)\n",
    "            \n",
    "        res[names[c]] = (np.mean(tmp),np.std(tmp))\n",
    "        c = c + 1 \n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def compute_cov_mfpt_give_file(file_name,p,fun,dist):\n",
    "\n",
    "    ori_cov,sta_cov,etn_cov,stm_cov,tag_cov,dym_cov = fun(file_name,p)\n",
    "\n",
    "    ec_kl = []\n",
    "    tc_kl = []\n",
    "    sc_kl = []\n",
    "    dc_kl = []\n",
    "\n",
    "    if dist == \"ks\":\n",
    "        for i in etn_cov:\n",
    "            ec_kl.append(ks_2samp(ori_cov,i)[0])\n",
    "        for i in tag_cov:\n",
    "            tc_kl.append(ks_2samp(ori_cov,i)[0])\n",
    "        for i in dym_cov:\n",
    "            dc_kl.append(ks_2samp(ori_cov,i)[0])\n",
    "        for i in stm_cov:\n",
    "            sc_kl.append(ks_2samp(ori_cov,i)[0])\n",
    "        stb = ks_2samp(ori_cov,sta_cov)[0]\n",
    "        \n",
    "    elif dist == \"js\":\n",
    "        for i in etn_cov:\n",
    "            ec_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=True))\n",
    "        for i in tag_cov:\n",
    "            tc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=True))\n",
    "        for i in dym_cov:\n",
    "            dc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=True))\n",
    "        for i in stm_cov:\n",
    "            sc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=True))\n",
    "            \n",
    "        stb = compute_kl_divergence(ori_cov,sta_cov, n_bins=50,js=True)\n",
    "    \n",
    "    elif dist == \"kl\":\n",
    "        for i in etn_cov:\n",
    "            ec_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=False))\n",
    "        for i in tag_cov:\n",
    "            tc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=False))\n",
    "        for i in dym_cov:\n",
    "            dc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=False))\n",
    "        for i in stm_cov:\n",
    "            sc_kl.append(compute_kl_divergence(ori_cov,i, n_bins=50,js=False))\n",
    "            \n",
    "        stb = compute_kl_divergence(ori_cov,sta_cov, n_bins=50,js=False)\n",
    "    \n",
    "        \n",
    "    elif dist == \"em\":\n",
    "        for i in etn_cov:\n",
    "            ec_kl.append(em(ori_cov,i))\n",
    "        for i in tag_cov:\n",
    "            tc_kl.append(em(ori_cov,i))\n",
    "        for i in dym_cov:\n",
    "            dc_kl.append(em(ori_cov,i))\n",
    "        for i in stm_cov:\n",
    "            sc_kl.append(em(ori_cov,i))\n",
    "        stb = em(ori_cov,sta_cov)\n",
    "    \n",
    "    else:\n",
    "        print(\"error\"*100)\n",
    "    \n",
    "    \n",
    "\n",
    "    res = [stb, np.mean(ec_kl),np.mean(tc_kl),\n",
    "           np.mean(sc_kl),np.mean(dc_kl)]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7049999999999998,\n",
       " 10.624100000000002,\n",
       " 36.62566666666667,\n",
       " 6.952500000000001,\n",
       " 15.902899999999999]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,0.23214387000094655,0.434527202885541,0.13664703515346613,0.39536672045404647,2.4849059218099367,7.3221755840003135,2.2964990082886954,8.71534618505758,10.624100000000002,36.62566666666667,6.952500000000001,15.902899999999999,0.5502,0.8933333333333332,0.3559,0.7175999999999999,\n",
      "_meta ,0.04177698541720046,0.058730623007760246,0.20300428374924567,0.28475297632186464,0.17778975889732726,0.3159365931622194,3.875254605114532,1.9340074039350927,1.0615,5.174499999999999,1.6025,4.757,0.11,0.259,0.273,0.6185,\n",
      "_2picco ,0.2629031745740039,0.21822898649270617,0.05395400268822901,0.6438754319299282,2.2773974288846137,0.8300988795481887,1.2957476379984025,15.932071484023737,10.9775,32.185,1.5935000000000001,21.618499999999997,0.6054999999999999,0.7545,0.16049999999999998,0.96,\n",
      " ,0.047144402605426405,0.03108927767816576,0.498741162315606,0.3727264701720285,0.24758607485556672,0.12987559779294597,3.826690651734915,4.5837218294820214,41.841245807914184,87.53680017933907,404.12367886946765,279.09937721711594,0.12094865670203722,0.15120628145813284,0.7324893486576893,0.49942432714090745,\n",
      "_meta ,0.07839958068444583,0.6451809053863446,0.3861345838977346,0.2666714261796327,0.8070476302544215,16.56254172197548,3.4168456077841234,1.059309313040754,21.202562473087703,109.63012985287756,108.24012631249147,49.69435528272439,0.09789117283314033,0.49301886009331886,0.5381930396015598,0.2877868623143275,\n",
      "_2picco ,0.056451018607534535,0.04361554264278332,0.39748668633943046,0.28885423421512796,0.2663022980131827,0.1784915028087276,3.244719018912104,1.145854960204233,40.812528312366226,48.15806233110007,183.32609276278404,78.01385754311414,0.11519402906695543,0.13569445693469143,0.5568171554976924,0.28224540137010157,\n"
     ]
    }
   ],
   "source": [
    "file_name = \"High_School11\"\n",
    "lambs_all = [\"la025\",\"la015\",\"la001\"]\n",
    "picchi = [\"\",\"_meta\",\"_2picco\"]\n",
    "\n",
    "\n",
    "for picco in picchi:\n",
    "    s = picco + \" ,\"\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_cov,\"js\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_cov,\"kl\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_cov,\"em\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "    \n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_cov,\"ks\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "    \n",
    "    print(s)\n",
    "    \n",
    "\n",
    "for picco in picchi:\n",
    "    s = picco + \" ,\"\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_mfpt,\"js\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_mfpt,\"kl\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "\n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_mfpt,\"em\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "    \n",
    "    stb,etn,tag,dym,stm = compute_cov_mfpt_give_file(file_name,picco,load_mfpt,\"ks\")\n",
    "    s += str(etn)+\",\"\n",
    "    s += str(tag)+\",\"\n",
    "    s += str(dym)+\",\"\n",
    "    s += str(stm)+\",\" \n",
    "    \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb17086194d7d73ddadbbf43f541071d992a7ee65541854f310228bfe926c80e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
