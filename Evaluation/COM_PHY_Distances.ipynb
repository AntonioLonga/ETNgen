{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import construction as cs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### read font\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_dirs = ['Barlow/']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "\n",
    "# set font\n",
    "plt.rcParams['font.family'] = 'Barlow'\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from topological_metrics import *\n",
    "\n",
    "import os\n",
    "\n",
    "# example of calculating the kl divergence between two mass functions\n",
    "from math import log2\n",
    "from scipy.stats import wasserstein_distance as em\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(data, n=10): \n",
    "    data = np.array(data)\n",
    "    h, e = np.histogram(data, n)\n",
    "    p = h/data.shape[0]\n",
    "    return e, p\n",
    "\n",
    "def support_intersection(p, q): \n",
    "    sup_int = (\n",
    "        list(\n",
    "            filter(\n",
    "                lambda x: (x[0]!=0) & (x[1]!=0), zip(p, q)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return sup_int\n",
    "\n",
    "def get_probs(list_of_tuples): \n",
    "    p = np.array([p[0] for p in list_of_tuples])\n",
    "    q = np.array([p[1] for p in list_of_tuples])\n",
    "    return p, q\n",
    "\n",
    "def kl_divergence(p, q): \n",
    "    return np.sum(p*np.log(p/q))\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = (1./2.)*(p + q)\n",
    "    return (1./2.)*kl_divergence(p, m) + (1./2.)*kl_divergence(q, m)\n",
    "\n",
    "def compute_kl_divergence(train_sample, test_sample, n_bins=10,js=False): \n",
    "    \"\"\"\n",
    "    Computes the KL Divergence using the support \n",
    "    intersection between two different samples\n",
    "    \"\"\"\n",
    "    E = 0.0000000001\n",
    "    e, p = compute_probs(train_sample, n=n_bins)\n",
    "    _, q = compute_probs(test_sample, n=e)\n",
    "    \n",
    "    p = np.array(p) + E \n",
    "    q = np.array(q) + E \n",
    "    \n",
    "    p = p/sum(p)\n",
    "    q = q/sum(q)\n",
    "    \n",
    "    list_of_tuples = support_intersection(p, q)\n",
    "    p, q = get_probs(list_of_tuples)\n",
    "    \n",
    "    if js:\n",
    "        return js_divergence(p, q)\n",
    "    else:\n",
    "        return kl_divergence(p, q)\n",
    "def comp_stat(ori,competitor,dist,names):\n",
    "    res = dict()\n",
    "    c = 0\n",
    "    for met in competitor:\n",
    "        tmp = []\n",
    "        for comp in met:\n",
    "            if dist == \"js\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=True)\n",
    "            elif dist == \"kl\":\n",
    "                val = compute_kl_divergence(ori[c], comp, n_bins=50,js=False)\n",
    "            elif dist == \"em\":\n",
    "                val = em(ori[c][0],comp)\n",
    "            elif dist == \"ks\":\n",
    "                val = (ks_2samp(ori[c][0],comp)[0])\n",
    "                \n",
    "            tmp.append(val)\n",
    "            \n",
    "        res[names[c]] = (np.mean(tmp),np.std(tmp))\n",
    "        c = c + 1 \n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_comp_metric(file_name,dist=\"ks\"):\n",
    "    o_in = load_topo_original(file_name)\n",
    "    e_in = load_topo_distributions(\"etngen\",file_name)\n",
    "    t_in = load_topo_distributions(\"taggen\",file_name)\n",
    "    d_in = load_topo_distributions(\"dymgen\",file_name)\n",
    "    s_in = load_topo_distributions(\"stmgen\",file_name)\n",
    "\n",
    "    res_e = comp_stat(o_in,e_in,dist= dist,names=names)\n",
    "    res_d = comp_stat(o_in,d_in,dist= dist,names=names)\n",
    "    res_s = comp_stat(o_in,s_in,dist= dist,names=names)\n",
    "    res_t = comp_stat(o_in,t_in,dist= dist,names=names)\n",
    "\n",
    "    x1 = np.array(list(res_e.values()))\n",
    "    x2 = np.array(list(res_s.values()))\n",
    "    x3 = np.array(list(res_t.values()))\n",
    "    x4 = np.array(list(res_d.values()))\n",
    "    \n",
    "    return x1,x2,x3,x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topo_distributions(generator,file_name):\n",
    "    \n",
    "    den = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/density.npy\",allow_pickle=True)\n",
    "    inter_indiv = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/interacting_indiv.npy\",allow_pickle=True)\n",
    "    new_conv = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/new_con.npy\",allow_pickle=True)\n",
    "    durat = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/dur.npy\",allow_pickle=True)\n",
    "    clust = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/clust.npy\",allow_pickle=True)\n",
    "    #s_met = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/s_metric.npy\",allow_pickle=True)\n",
    "    ass = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/ass.npy\",allow_pickle=True)\n",
    "    #asp = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/asp.npy\",allow_pickle=True)\n",
    "    hclose = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hclose.npy\",allow_pickle=True)\n",
    "    hbet = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hbet.npy\",allow_pickle=True)\n",
    "    whbet = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/whbet.npy\",allow_pickle=True)\n",
    "    conncomp = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/conncomp.npy\",allow_pickle=True)\n",
    "    hmod = np.load(\"topology_results/topology_results_giulia/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hmod.npy\",allow_pickle=True)\n",
    "    \n",
    "    hs_met = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hs_metric.npy\",allow_pickle=True)\n",
    "    hasp = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/hasp.npy\",allow_pickle=True)\n",
    "    #nb_inter = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/nb_interactions.npy\",allow_pickle=True)\n",
    "    #stren = np.load(\"topology_results/\"+generator+\"/Multiple_run/distributions/\"+file_name+\"/streng.npy\",allow_pickle=True \n",
    "\n",
    "    #return den,inter_indiv,new_conv,durat,clust,ass,hclose,hbet,whbet,conncomp,hmod,hs_met,hasp\n",
    "    return den,inter_indiv,new_conv,durat,clust,ass,conncomp,hclose,hbet,whbet,hmod,hs_met,hasp\n",
    "\n",
    "def load_topo_original(file_name):\n",
    "    den = np.load(\"topology_results/original_distributions/\"+file_name+\"/density.npy\",allow_pickle=True)\n",
    "    inter_indiv = np.load(\"topology_results/original_distributions/\"+file_name+\"/interacting_indiv.npy\",allow_pickle=True)\n",
    "    new_conv = np.load(\"topology_results/original_distributions/\"+file_name+\"/new_con.npy\",allow_pickle=True)\n",
    "    durat = np.load(\"topology_results/original_distributions/\"+file_name+\"/dur.npy\",allow_pickle=True)\n",
    "    clust = np.load(\"topology_results/original_distributions/\"+file_name+\"/clust.npy\",allow_pickle=True)\n",
    "    ass = np.load(\"topology_results/original_distributions/\"+file_name+\"/ass.npy\",allow_pickle=True)\n",
    "    hclose = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hclose.npy\",allow_pickle=True)\n",
    "    hbet = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hbet.npy\",allow_pickle=True)\n",
    "    whbet = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/whbet.npy\",allow_pickle=True)\n",
    "    conncomp = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/conncomp.npy\",allow_pickle=True)\n",
    "    hmod = np.load(\"topology_results/topology_results_giulia/original_distributions/\"+file_name+\"/hmod.npy\",allow_pickle=True)\n",
    "    \n",
    "    hs_met = np.load(\"topology_results/original_distributions/\"+file_name+\"/hs_metric.npy\",allow_pickle=True)\n",
    "    hasp = np.load(\"topology_results/original_distributions/\"+file_name+\"/hasp.npy\",allow_pickle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #s_met = np.load(\"topology_results/original_distributions/\"+file_name+\"/s_metric.npy\",allow_pickle=True)\n",
    "    #asp = np.load(\"topology_results/original_distributions/\"+file_name+\"/asp.npy\",allow_pickle=True)\n",
    "    #stren = np.load(\"topology_results/original_distributions/\"+file_name+\"/streng.npy\",allow_pickle=True)\n",
    "    #nb_inter = np.load(\"topology_results/original_distributions/\"+file_name+\"/nb_interactions.npy\",allow_pickle=True)\n",
    "    \n",
    "    return den,inter_indiv,new_conv,durat,clust,ass,[conncomp],[hclose],[hbet],[whbet],[hmod],[hs_met],[hasp]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Density\",\"Interacting  individuals\",\"New conversations\",\"Duration of contacts\",\"Global clustering coefficient\",\n",
    "         \"Assortativity\",\"Nb connected components\",\"Hour closeness\",\"Hour betweenness\",\"Weighted hour betweenness\",\n",
    "         \"Hour modularity\",\"Hour S-metric\",\"Hour average shortestpath length\"]\n",
    "\n",
    "# etn stm tag dym\n",
    "#x1w,x2w,x3w,x4w = load_comp_metric(\"InVS13\",dist=dist)\n",
    "#x1s,x2s,x3s,x4s = load_comp_metric(\"High_School11\",dist=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name  = \"High_School11\"\n",
    "\n",
    "\n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"ks\")\n",
    "tmp = []\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp.append([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"js\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    cc = cc + 1\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"kl\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.2f}\".format(a),\"{:.2f}\".format(b),\"{:.2f}\".format(c),\"{:.2f}\".format(d)])\n",
    "    cc = cc + 1\n",
    "    \n",
    "x1h,x2h,x3h,x4h = load_comp_metric(file_name,dist=\"em\")\n",
    "cc = 0\n",
    "for i in range(len(x1h)):\n",
    "    a = x1h[i][0]\n",
    "    b = x2h[i][0]\n",
    "    c = x3h[i][0]\n",
    "    d = x4h[i][0]\n",
    "    tmp[cc].extend([\"{:.5f}\".format(a),\"{:.5f}\".format(b),\"{:.5f}\".format(c),\"{:.5f}\".format(d)])\n",
    "    cc = cc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "names2 = [\"Density \",\"Int. ind.\",\"New conv.\",\"Dur.\",\"GCC\",\"Ass.\",\"Con. com.\",\"H. clos.\",\"H betw.\",\"W. h. betw.\",\n",
    "\"H. modu.\"   ,\n",
    "\"H. S-met.\"  ,\n",
    "\"H. aspl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density &0.09 & 0.55 & 0.28 & 0.30 & 0.03 & 0.18 & 0.10 & 0.16 & 0.56 & 8.03 & 0.84 & 3.26 & 0.00043 & 0.00112 & 0.00503 & 0.00117 \\\\\n",
      "Int. ind.&0.09 & 0.56 & 0.28 & 0.27 & 0.03 & 0.19 & 0.10 & 0.08 & 0.56 & 3.33 & 0.84 & 1.66 & 3.40660 & 7.84999 & 39.64996 & 6.18392 \\\\\n",
      "New conv.&0.16 & 0.27 & 0.66 & 0.18 & 0.07 & 0.09 & 0.31 & 0.07 & 1.67 & 3.35 & 4.87 & 2.55 & 5.56380 & 8.31258 & 74.41332 & 6.98200 \\\\\n",
      "Dur.&0.24 & 0.38 & 0.20 & 0.37 & 0.06 & 0.14 & 0.04 & 0.14 & 0.34 & 2.94 & 0.28 & 4.02 & 0.53687 & 0.38945 & 0.23613 & 0.38596 \\\\\n",
      "GCC&0.14 & 0.08 & 0.18 & 0.13 & 0.07 & 0.05 & 0.13 & 0.05 & 1.59 & 1.25 & 1.90 & 0.78 & 0.05790 & 0.04507 & 0.05844 & 0.09096 \\\\\n",
      "Ass.&0.33 & 0.70 & 0.54 & 0.32 & 0.13 & 0.45 & 0.26 & 0.18 & 2.25 & 8.70 & 5.98 & 1.43 & 0.24615 & 0.41422 & 0.36310 & 0.23898 \\\\\n",
      "Con. com.&0.10 & 1.00 & 0.28 & 0.99 & 0.04 & 0.34 & 0.10 & 0.61 & 0.53 & 1.93 & 1.24 & 16.83 & 2.28525 & 247.55778 & 15.54240 & 63.70242 \\\\\n",
      "H. clos.&0.22 & 0.49 & 0.40 & 0.58 & 0.14 & 0.43 & 0.27 & 0.30 & 3.78 & 14.61 & 7.71 & 4.50 & 0.05229 & 0.05471 & 0.12078 & 0.19681 \\\\\n",
      "H betw.&0.21 & 0.50 & 0.20 & 0.55 & 0.10 & 0.13 & 0.16 & 0.29 & 1.47 & 2.99 & 3.73 & 2.25 & 0.00801 & 0.00676 & 0.00551 & 0.03840 \\\\\n",
      "W. h. betw.&0.21 & 0.50 & 0.19 & 0.52 & 0.09 & 0.14 & 0.16 & 0.27 & 1.67 & 3.49 & 3.72 & 2.13 & 0.00918 & 0.00738 & 0.00619 & 0.03766 \\\\\n",
      "H. modu.&0.29 & 0.46 & 0.73 & 0.46 & 0.31 & 0.27 & 0.52 & 0.35 & 7.95 & 6.94 & 15.21 & 8.61 & 0.08775 & 0.14279 & 0.34414 & 0.17921 \\\\\n",
      "H. S-met.&0.12 & 0.56 & 0.35 & 0.42 & 0.07 & 0.08 & 0.10 & 0.09 & 1.84 & 3.67 & 2.92 & 2.65 & 2603.68947 & 4061.54375 & 596631.13622 & 3572.40038 \\\\\n",
      "H. aspl&0.18 & 0.56 & 0.23 & 0.44 & 0.14 & 0.42 & 0.22 & 0.29 & 3.44 & 13.68 & 6.05 & 3.68 & 0.43245 & 1.65950 & 0.42850 & 1.15358 \\\\\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in tmp:\n",
    "    s = names2[c]+\"&\"\n",
    "    for j in i:\n",
    "        s = s + j + \" & \"\n",
    "    c = c +1 \n",
    "    print(s[:-2]+\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density\n",
      "Interacting  individuals\n",
      "New conversations\n",
      "Duration of contacts\n",
      "Global clustering coefficient\n",
      "Assortativity\n",
      "Nb connected components\n",
      "Hour closeness\n",
      "Hour betweenness\n",
      "Weighted hour betweenness\n",
      "Hour modularity\n",
      "Hour S-metric\n",
      "Hour average shortestpath length\n"
     ]
    }
   ],
   "source": [
    "for i in names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.08 & 0.07 & 0.05 & 0.38 \\\\\n",
    "0.09 & 0.07 & 0.06 & 0.37 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb17086194d7d73ddadbbf43f541071d992a7ee65541854f310228bfe926c80e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
